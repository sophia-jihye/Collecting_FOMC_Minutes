{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, re, pickle, argparse, shutil\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import requests\n",
    "from daterangeparser import parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "start_mmddyyyy = \"01/01/1993\"\n",
    "end_mmddyyyy = \"08/22/2023\"\n",
    "\n",
    "chromedriver_filepath = \"C:\\GIT\\SELENIUM_DRIVERS\\chromedriver-win64\\chromedriver.exe\"\n",
    "save_root_dir = './Minutes'\n",
    "\n",
    "url = \"https://www.federalreserve.gov/monetarypolicy/materials/\"\n",
    "\n",
    "\n",
    "def extract_begin_end_dates(date_range):\n",
    "    if '-' not in date_range:\n",
    "        parsed, _ = parse(date_range)\n",
    "        return parsed, parsed\n",
    "    \n",
    "    elif '/' in date_range:\n",
    "        begin_month, end_month, begin_date, end_date, year = date_range.replace(',', '').replace('-', ' ').replace('/', ' ').split(' ')\n",
    "        date_range = f'{begin_month} {begin_date}-{end_month} {end_date}, {year}'\n",
    "        return parse(date_range)\n",
    "        \n",
    "    else:\n",
    "        return parse(date_range)\n",
    "\n",
    "\n",
    "def prepare_resources_for_scraping(chromedriver_filepath, url, start_mmddyyyy, end_mmddyyyy):\n",
    "    driver = webdriver.Chrome(chromedriver_filepath)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # set start date\n",
    "    start_date = driver.find_element_by_name(\"startmodel\")\n",
    "    start_date.clear()\n",
    "    start_date.send_keys(start_mmddyyyy)\n",
    "\n",
    "    # set end date\n",
    "    end_date = driver.find_element_by_name(\"endmodel\")\n",
    "    end_date.clear()\n",
    "    end_date.send_keys(end_mmddyyyy)\n",
    "\n",
    "    # select items\n",
    "    xpath_strings = \"//label/input[contains(..,'Minutes (1993-Present)')]\"\n",
    "    minute_checkbox = driver.find_element_by_xpath(xpath_strings)\n",
    "    minute_checkbox.click()\n",
    "\n",
    "    # apply filter\n",
    "    submit = driver.find_element_by_css_selector(\".btn.btn-primary\")\n",
    "    submit.click()\n",
    "    \n",
    "    # get the page control row\n",
    "    pagination = driver.find_element_by_class_name('pagination')\n",
    "\n",
    "    # go to the last page to find the largest page number\n",
    "    last_page = pagination.find_element_by_link_text('Last')\n",
    "    last_page.click()\n",
    "    pages = pagination.text.split('\\n')\n",
    "    largest_page = int(pages[-3])\n",
    "    \n",
    "    return driver, pagination, largest_page\n",
    "\n",
    "def extract_meetingdate_documentdate_minuteurl(soup):\n",
    "    meeting_date = soup.select('strong')[0].text\n",
    "    document_date = soup.select('em')[0].text\n",
    "    minute_url = 'https://www.federalreserve.gov/{}'.format([item for item in soup.select('a') if 'HTML' in item.text][0]['href'])\n",
    "    return meeting_date, document_date, minute_url\n",
    "\n",
    "def scrape_URLs_and_meeting_dates_and_document_dates(driver, pagination, largest_page):\n",
    "    meeting_date_list, document_date_list, minute_url_list = [], [], []\n",
    "    # go back to first page and start the loop\n",
    "    first_page = pagination.find_element_by_link_text('First')\n",
    "    first_page.click()\n",
    "    next_page = pagination.find_element_by_link_text('Next')\n",
    "    \n",
    "    for _ in range(largest_page):\n",
    "        driver.find_element_by_css_selector(\".panel.panel-default\") \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        rows = soup.select('div.row.fomc-meeting')[1:]\n",
    "        for one_row in rows:\n",
    "            try:\n",
    "                if one_row.select('.fomc-meeting__month.col-xs-5.col-sm-3.col-md-4')[0].text.strip()=='Minutes':\n",
    "                    # Extract minutes written in HTML format\n",
    "                    meeting_date, document_date, minute_url = extract_meetingdate_documentdate_minuteurl(one_row)\n",
    "                    meeting_date_list.append(meeting_date)\n",
    "                    document_date_list.append(document_date)\n",
    "                    minute_url_list.append(minute_url)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        next_page.click()\n",
    "    print('Number of URLs: {}'.format(len(minute_url_list)))\n",
    "    \n",
    "    return minute_url_list, meeting_date_list, document_date_list\n",
    "\n",
    "def get_text_for_a_minute_from_201201_to_202209(soup):\n",
    "    return soup.find('div', class_ = 'col-xs-12 col-sm-8 col-md-9').text.strip()\n",
    "\n",
    "def get_text_for_a_minute_from_200710_to_201112(soup):\n",
    "    return soup.find('div', id=\"leftText\").text.strip()\n",
    "\n",
    "def get_text_for_a_minute_from_199601_to_200709(soup):\n",
    "    return '\\n'.join([item.text.strip() for item in soup.select('table td')])\n",
    "\n",
    "def get_text_for_a_minute_from_199401_to_199512(soup):\n",
    "    return soup.find('div', id=\"content\").text.strip()\n",
    "\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "def remove_doublespaces(document):\n",
    "    return doublespace_pattern.sub(' ', document).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs: 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [03:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 254 unique documents under ./Minutes\n",
      "Created ScrapingErrors.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "driver, pagination, largest_page = prepare_resources_for_scraping(chromedriver_filepath, url, start_mmddyyyy, end_mmddyyyy)\n",
    "minute_url_list, meeting_date_list, document_date_list = scrape_URLs_and_meeting_dates_and_document_dates(driver, pagination, largest_page)\n",
    "\n",
    "doc_count = 0\n",
    "error_list = []\n",
    "for minute_url, meeting_date, document_date in tqdm(zip(minute_url_list, meeting_date_list, document_date_list)):\n",
    "\n",
    "    # Scrape minutes\n",
    "    minute_resp = requests.get(minute_url)\n",
    "    minute_soup = BeautifulSoup(minute_resp.content, 'lxml')\n",
    "\n",
    "    document_date_yyyymmdd = datetime.strftime(datetime.strptime(document_date, \"%B %d, %Y\"), \"%Y%m%d\")\n",
    "    yearmonth = int(document_date_yyyymmdd[:6])\n",
    "    try:\n",
    "        if yearmonth >= 201201:\n",
    "            doc = get_text_for_a_minute_from_201201_to_202209(minute_soup)\n",
    "        elif yearmonth >= 200710:\n",
    "            doc = get_text_for_a_minute_from_200710_to_201112(minute_soup)\n",
    "        elif yearmonth >= 199601:\n",
    "            doc = get_text_for_a_minute_from_199601_to_200709(minute_soup)    \n",
    "        else:\n",
    "            doc = get_text_for_a_minute_from_199401_to_199512(minute_soup)\n",
    "    except:\n",
    "        error_list.append((minute_url, meeting_date, document_date))\n",
    "        continue\n",
    "\n",
    "    # Clean\n",
    "    doc = remove_doublespaces(doc)\n",
    "\n",
    "    meeting_date_start, meeting_date_end = extract_begin_end_dates(meeting_date)\n",
    "    meeting_date_start_string = meeting_date_start.strftime(\"%Y-%m-%d\")\n",
    "    meeting_date_end_string = meeting_date_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Save data\n",
    "    save_dir = os.path.join(save_root_dir, document_date_yyyymmdd[:4])\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "    save_filepath = os.path.join(save_dir, 'MeetingDate={}-{}_UploadedOn={}.txt'\\\n",
    "                                 .format(meeting_date_start_string, meeting_date_end_string, document_date_yyyymmdd))\n",
    "    with open(save_filepath, \"w\", encoding='utf-8-sig') as file:\n",
    "        file.write(doc)\n",
    "        doc_count += 1\n",
    "\n",
    "print('Saved {} unique documents under {}'.format(len(glob('{}/*/*.txt'.format(save_root_dir))), save_root_dir)) \n",
    "\n",
    "# Save errors\n",
    "if len(error_list) > 0:\n",
    "    save_filepath = os.path.join('ScrapingErrors.csv')\n",
    "    pd.DataFrame(error_list, columns=['url', 'meeting_date', 'document_date']).to_csv(save_filepath, index=False)\n",
    "    print('Created {}'.format(save_filepath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
